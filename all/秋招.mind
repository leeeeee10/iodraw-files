{"root":{"data":{"id":"demnmynpvpc0","created":1764569372141,"text":"秋招"},"children":[{"data":{"id":"demjk7q71940","created":1764557872349,"text":"语言方面","expandState":"expand"},"children":[{"data":{"id":"demjke1f35s0","created":1764557886089,"text":"python","expandState":"expand"},"children":[{"data":{"id":"demjki9c4r40","created":1764557895275,"text":"库：调用大模型的时候用的什么库，是sdk还是openai","layout_right_offset":{"x":2,"y":-62},"expandState":"expand","note":"### OpenAI 官方 SDK (openai)\n- **安装**：`pip install openai`\n- **使用**：OpenAI 提供的官方 Python 库，用于与 GPT 系列模型进行交互。\n- **功能**：直接访问 OpenAI 提供的 GPT、DALL·E、Whisper 等模型。\n- **特点**：\n  1. 简单易用，提供封装良好的 API 调用\n  2. 需要 API 密钥进行认证\n  3. 支持调节模型参数（如温度、max tokens 等）\n\n### 自定义 SDK\n- **说明**：部分公司或组织为满足特定需求或集成场景开发的专属 SDK，例如 Hugging Face、Anthropic（Claude）、Cohere 等平台均提供自有模型的调用 SDK。\n- **特点**：\n  1. 针对特定平台定制化，支持更复杂的模型管理和训练功能\n  2. 可能需要处理更多底层细节（如模型部署、微调等）\n\n### 核心区别\n| 对比维度       | OpenAI 官方 SDK                          | 自定义 SDK                              |\n|----------------|------------------------------------------|-----------------------------------------|\n| 开发主体       | OpenAI 官方                               | 第三方平台（如 Hugging Face、Anthropic） |\n| 核心定位       | 专注自家模型调用，追求简单高效            | 适配多平台/特定场景，支持深度自定义      |\n| 使用门槛       | 低，适合快速上手                          | 较高，需了解底层细节                    |\n| 功能侧重       | 模型调用标准化，参数调节便捷              | 模型训练、部署、定制化开发              |"},"children":[]},{"data":{"id":"demjl2qho4o0","created":1764557939848,"text":"python的io了解吗","expandState":"expand","note":"io 模块常见的应用包括文件操作、内存缓冲、数据流处理等。"},"children":[]},{"data":{"id":"demjl73tzc00","created":1764557949361,"text":"描述下你项目里面的python的多线程和批次处理是如何设计的","expandState":"expand","note":"在项目中，我们需要处理上百万条数据，因此采用了多线程与批次处理结合的方式来提升吞吐量。\n\n### 核心设计要点\n#### 1. 批次处理：控制内存与任务粒度\n- 本地开发环境：每批固定 4 条，便于调试与观察日志。\n- 线上环境：批次大小根据数据量与资源动态计算（如 50/100/500 条），避免过多任务切分带来的调度开销。\n\n#### 2. 多线程：提升处理速度\n- 开发环境：固定 4 个工作线程，便于开发机资源受限时稳定运行。\n- 生产环境：线程池动态伸缩，根据 CPU 核心数和队列堆积情况自动增加或减少线程数量。\n\n### 高吞吐策略\n数据在“批次 + 多线程”的双重加速下，整体处理性能有明显提升：\n- 减少数据库连接次数\n- 降低上下文切换\n- 提高 CPU 利用率\n\n### 一句总结\n我通过批次处理提升单轮处理量，通过多线程提升并发能力，并在生产环境实现动态伸缩，使上百万级数据处理规模可以在可控时间内完成。"},"children":[]},{"data":{"id":"demjmiufx680","created":1764558053279,"text":"项目里面的生产者-消费者你是怎么设计的，描述一下这个过程","expandState":"expand","note":"### 高吞吐生产者-队列-多线程消费者流水线设计\n为处理百万级数据，通过“任务批次拆分 + 线程安全队列 + 多线程并发消费”构建稳定流水线，实现高吞吐、解耦与限流的核心目标。\n\n---\n\n#### 1. 生产者（Producer）\n- **核心职责**：数据供给与批次打包\n  - 批量从数据源读取数据（支持数据库、消息队列、文件等）\n  - 将数据按预设批次大小打包，塞入线程安全队列\n  - 动态控制生产速度，避免队列堆积溢出\n\n---\n\n#### 2. 消费者（Consumer）\n- **核心职责**：并发处理与批量输出\n  - 基于多线程模型，从队列中并发取出数据批次\n  - 执行核心业务逻辑（数据清洗、格式转换、接口调用等）\n  - 处理完成后批量写入数据库或下游系统，提升写入效率\n\n---\n\n#### 3. 队列（Queue）\n- **核心特性**：线程安全 + 缓冲限流\n  - 天然支持线程安全，避免多线程并发访问冲突\n  - 作为缓冲层，解耦生产者抓取速度与消费者处理/入库速度差异\n  - 智能限流机制：\n    - 队列满时 → 阻塞生产者，防止数据溢出\n    - 队列空时 → 阻塞消费者，等待数据供给\n\n---\n\n#### 设计优势\n1. 解耦隔离：避免入库/处理速度过慢拖垮数据抓取过程，各模块独立伸缩\n2. 高吞吐高效：多线程并发消费充分利用硬件资源，批次处理减少频繁IO开销\n3. 稳定可控：通过队列缓冲均衡整体吞吐量，避免流量峰值冲击\n4. 高可用支撑：便于扩展失败重试、数据补偿、异常监控等容错机制\n5. 资源优化：有效利用多线程并发能力，提升CPU与IO利用率"},"children":[]},{"data":{"id":"demo1iy41n40","created":1764570513403,"text":"你的消息队列是用的什么？","note":"### 队列选型：采用 Python 内置 `queue.Queue` 而非外部消息队列\n在生产者-消费者流水线中，我们未选用 Kafka、RabbitMQ 等外部消息队列系统，而是直接采用 Python 标准库内置的 **`queue.Queue`** 作为任务调度与缓冲核心，适配百万级批处理场景的需求。\n\n---\n\n#### 选型核心原因\n1. **场景适配：内部同步处理，无需跨场景传递**\n   - 任务均为单服务内部的同步批处理，无需跨进程、跨服务器或跨服务传递数据，`queue.Queue` 完全满足本地线程间通信需求。\n2. **部署轻量化：降低复杂度，随进程启停**\n   - 无需额外部署、维护外部消息队列服务，`queue.Queue` 随应用进程启动，无额外运维成本，适配单服务批处理的极简部署需求。\n3. **性能足够：线程安全+高吞吐支撑**\n   - 原生支持线程安全，无需额外封装锁机制，避免多线程并发访问冲突；\n   - 针对百万级任务吞吐场景，其缓冲与调度性能完全够用，无需过度设计。"},"children":[]},{"data":{"id":"demjlrrryuw0","created":1764557994345,"text":"项目里面你有用到tensorflow吗，具体用到了哪些请你描述一下","expandState":"expand","note":"### TensorFlow 在项目中的使用\n\n在项目中，我使用 TensorFlow 主要用于 **模型推理和数据处理任务**，具体应用如下：\n\n---\n\n#### 1. 模型加载与推理\n- 使用 `tf.saved_model.load()` 加载训练好的模型\n- 使用 `model(inputs)` 或 `model.predict()` 对批量数据进行推理\n- 对输出结果进行后处理（如分类标签映射、概率阈值筛选）\n\n```python\nimport tensorflow as tf\n\n# 加载模型\nmodel = tf.saved_model.load(\"saved_model/my_model\")\n\n# 批量推理\npredictions = model(tf.constant(batch_inputs))\n````\n\n---\n\n#### 2. 数据预处理与批次处理\n\n* 利用 `tf.data.Dataset` 构建高效数据 pipeline\n* 支持批次化（batch）、缓存（cache）和预取（prefetch）\n* 与多线程或异步 pipeline 配合，提高处理吞吐量\n\n```python\ndataset = tf.data.Dataset.from_tensor_slices(raw_data)\ndataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n```\n\n---\n\n#### 3. GPU 加速\n\n* 使用 TensorFlow 的 GPU 支持，加速大规模数据推理\n* 通过 `tf.config.list_physical_devices('GPU')` 检查可用 GPU\n* 自动在 GPU 上执行张量计算，提升性能\n\n---\n\n#### 一句话总结（面试可用）\n\n我在项目中使用 TensorFlow 加载训练好的模型进行批量数据推理，结合 `tf.data` pipeline 做高效批处理，并利用 GPU 加速，实现百万级数据的快速处理和模型推理。\n"},"children":[]},{"data":{"id":"denin57ysww0","created":1764656840833,"text":"百万级 Parquet + LLM API 多进程处理性能下降分析与优化","note":"#### 一、问题根因分析\n\n1. **性能从 40 条/s 降到 7 条/s 并卡住**\n   - **并发过高导致系统过载**\n     - 多进程参数远超服务器硬件承载（如 CPU 上下文切换激增）\n   - **API 调用未复用连接**\n     - 每次请求都新建 TCP 连接，端口逐渐耗尽\n   - **平台隐形限流**\n     - 并发超过配额时不会立即报错，而是**延迟响应** → 请求排队 → 吞吐量持续下降\n\n2. **冲高到 17 条/s 但回落并稳定到 10 条/s**\n   - 达到平台真实吞吐上限后\n   - API **动态限流机制生效**：延长响应时间以锁定吞吐量上限\n\n---\n\n#### 二、解决思路\n\n核心策略：**并发不盲拉满 → 合理匹配硬件与平台上限 → 提高单请求效率**\n\n---\n\n#### 三、具体优化措施与对应效果\n\n| 问题 | 优化措施 | 效果 |\n|------|---------|------|\n| 系统过载 | 多进程数按 CPU **1/4 核心数**配置 | 减少上下文切换，性能稳定 |\n| 端口耗尽 | 启用 **HTTP Session 连接池** | 避免资源泄露，效率翻倍 |\n| API 等待过长 | 降低超时时间/重试次数；降低 max_tokens | 降低排队与超时风险 |\n| 队列阻塞 | 增大 result queue maxsize | 避免处理线程互锁 |\n| IO 频繁 | parquet **批次增大至 500 条** | 减少数据加载开销 |\n| 单进程吞吐不足 | 每进程+线程池（2 workers） | 增加实质并行数量 |\n| 平台限流触发 | 控制活跃连接不超过**平台上限** | 长时间运行保持稳定 |\n\n---\n\n#### 四、优化后的量化结果\n\n| 项目 | 优化前 | 优化后 |\n|------|------|------|\n| 峰值吞吐 | 40 条/s | 17 条/s |\n| 稳定吞吐 | 7 条/s | **10-17 条/s 稳定运行** |\n| 内存/端口泄露 | 有 | 无 |\n| 运行时长 | 无法跑完 | 百万级数据可在可控时间内完成 |\n\n系统资源利用率：\n- CPU：70%~90% 稳定\n- 内存无累积上涨\n\n---\n\n#### 五、核心总结（面试一口气答版）\n\n**吞吐量不是并发越高越好。**  \n性能下降的根因是：**并发超硬件能力 + API 隐形限流 + TCP 连接资源泄露**。  \n我通过：\n- 限并发、稳连接、提批次\n- 单进程轻线程并行\n- 避免触发平台动态限流\n\n最终让任务吞吐量保持**长期稳定**，成功完成百万级 LLM 批量推理任务。\n\n---\n\n如面试官追问，我可以补充：\n- 队列阻塞检测与 backpressure 控制\n- LLM 请求失败自动“熔断+降级+重试”策略\n- API RTT 监控 + 指标驱动动态扩缩容机制\n\n可获得更高技术深度认可。\n"},"children":[]}]},{"data":{"id":"demjn2p2mqo0","created":1764558096490,"text":"java","expandState":"expand"},"children":[{"data":{"id":"demjn52prg00","created":1764558101669,"text":"没被问到过","expandState":"expand"},"children":[]}]},{"data":{"id":"demjn87g7c80","created":1764558108485,"text":"vue","expandState":"expand"},"children":[{"data":{"id":"demjqwoftkg0","created":1764558396848,"text":"vue3和之前版本的区别","expandState":"expand","note":"### Vue 3 与 Vue 2 的主要区别\n\nVue 3 是对 Vue 框架的重写升级版本，在性能、逻辑组织和 TypeScript 支持等方面都有显著改进。\n\n---\n\n#### 1. 核心 API 改进\n\n| 方面 | Vue 2 | Vue 3 |\n|------|-------|-------|\n| **响应式系统** | 基于 `Object.defineProperty` | 基于 Proxy，实现更完整和高效的响应式 |\n| **Composition API** | 不支持 | 支持，可用 `setup()` 组织逻辑，方便复用和组合 |\n| **Options API** | 默认使用 | 仍支持，但建议组合使用 Composition API |\n| **生命周期钩子** | `created`, `mounted` 等 | 新增 `onMounted`, `onUnmounted` 等组合式钩子 |\n| **性能** | 较好 | 重写虚拟 DOM，更快，包体积更小 |\n| **Tree Shaking** | 不支持 | 支持，按需引入减少打包体积 |\n\n---\n\n#### 2. TypeScript 支持\n\n- Vue 3 对 TypeScript 支持友好，官方声明了完整类型\n- 组件、Props、Emits 都可以完全类型推导\n- Vue 2 对 TS 支持不完善，需要第三方库补充类型\n\n---\n\n#### 3. 其他优化特性\n\n- **Fragment**：支持组件返回多个根节点\n- **Teleport**：可以将组件渲染到 DOM 的任意位置\n- **Suspense**：支持异步组件加载，提供等待占位和错误处理\n- **更小的体积**：Vue 3 核心比 Vue 2 小约 50%，渲染性能提升 2-3 倍\n- **更快的更新**：虚拟 DOM 重写、编译优化，减少不必要的组件更新\n\n---\n\n#### 4. 总结\n\nVue 3 的优势主要体现在：\n\n1. **性能提升**：Proxy 响应式 + 虚拟 DOM 重写\n2. **逻辑复用更方便**：Composition API\n3. **TypeScript 支持更好**\n4. **现代特性丰富**：Fragment, Teleport, Suspense 等\n5. **包体积更小，Tree Shaking 支持**\n\n一句话概括：\nVue 3 是对 Vue 2 的全面升级，解决了逻辑复用、性能和 TypeScript 支持的问题，同时提供了更多现代化前端特性。\n"},"children":[]}]},{"data":{"id":"demkwhrf1r40","created":1764561655671,"text":"C++","expandState":"expand"},"children":[{"data":{"id":"demkwjm3n2w0","created":1764561659703,"text":"讲一下c++的编译原理和流程","expandState":"expand","note":"### C++ 编译原理与流程\n\nC++ 的编译是一个**多阶段的过程**，涉及源代码到可执行文件的转换，每个阶段都有明确职责。\n\n---\n\n#### 1. 预处理阶段（Preprocessing）\n- 处理以 `#` 开头的指令，如：\n  - `#include`：包含头文件\n  - `#define`：宏替换\n  - 条件编译指令（`#ifdef`, `#ifndef` 等）\n- 输出结果是**纯 C++ 代码**（没有宏和头文件指令），交给编译器进行下一步\n\n```text\n源文件.cpp → 预处理 → 扩展后的源代码\n````\n\n---\n\n#### 2. 编译阶段（Compilation）\n\n* 将预处理后的源代码翻译成汇编代码（机器可读指令的中间表示）\n* 语法和语义检查：\n\n  * 类型检查\n  * 语法错误\n  * 作用域和符号解析\n* 输出 **汇编代码文件**（通常 `.s` 文件）\n\n```text\n扩展后的源代码 → 编译 → 汇编代码\n```\n\n---\n\n#### 3. 汇编阶段（Assembly）\n\n* 将汇编代码翻译成**目标文件（object file）**\n* 目标文件包含机器码和符号表，但未完成链接\n* 输出 `.o` 或 `.obj` 文件\n\n```text\n汇编代码 → 汇编器 → 目标文件(.o/.obj)\n```\n\n---\n\n#### 4. 链接阶段（Linking）\n\n* 将多个目标文件和库文件链接成可执行文件\n* 任务包括：\n\n  * 符号解析：将函数、全局变量的引用与定义匹配\n  * 地址重定位：确定各代码段和数据段在内存中的最终地址\n  * 静态库合并：如果使用了 `.lib` 或 `.a` 库\n* 输出最终可执行文件（如 `a.out` 或 `program.exe`）\n\n```text\n目标文件 + 库 → 链接器 → 可执行文件\n```\n\n---\n\n#### 5. 执行阶段（Execution）\n\n* 可执行文件被加载到内存\n* 操作系统分配运行环境\n* CPU 执行机器码\n\n---\n\n#### 6. 总结\n\nC++ 编译流程可以概括为：\n\n```text\n源代码.cpp \n    → 预处理（Preprocessing） \n    → 编译（Compilation） \n    → 汇编（Assembly） \n    → 链接（Linking） \n    → 可执行文件 → 执行\n```\n\n关键点：\n\n* 预处理：宏展开、头文件合并\n* 编译：语法分析、语义检查、生成汇编\n* 汇编：汇编 → 目标文件\n* 链接：符号解析 + 地址重定位 + 静态库整合\n* 执行：操作系统加载运行\n\n一句话总结：\nC++ 编译是从源代码到可执行文件的多阶段转化，每一阶段有明确职责，从代码解析、机器码生成，到符号链接和执行。\n\n"},"children":[]},{"data":{"id":"demkwrgci600","created":1764561676770,"text":"软连接是什么，还有什么是和他相关的","expandState":"expand","note":"### 软连接（Symbolic Link）\n\n#### 1. 定义\n- **软连接（Symbolic Link / symlink）**是一个特殊类型的文件，它包含了另一个文件或目录的路径。\n- 访问软连接时，系统会自动跳转到目标文件或目录。\n\n#### 2. 特点\n- 类似于快捷方式，而不是文件本身\n- 可以跨文件系统\n- 如果目标文件被删除或移动，软连接会失效（成为“悬挂链接”）\n- 可以对目录和文件都创建软连接\n\n#### 3. 常用命令\n```bash\n# 创建软连接\nln -s /path/to/original /path/to/link\n\n# 查看链接指向\nls -l /path/to/link\n\n# 删除软连接（不影响原文件）\nrm /path/to/link\n````\n\n#### 4. 与软连接相关的概念\n\n| 名称                                  | 定义                           | 区别                                   |\n| ----------------------------------- | ---------------------------- | ------------------------------------ |\n| **硬连接（Hard Link）**                  | 直接指向文件的 inode，相当于给同一文件创建多个名字 | 必须在同一文件系统；删除一个硬链接，文件数据仍存在；不能对目录创建硬链接 |\n| **符号链接（Symbolic Link / Soft Link）** | 文件中存储目标路径                    | 可以跨文件系统；对目录有效；目标删除后链接失效              |\n| **悬挂链接（Dangling Link）**             | 目标文件不存在的软连接                  | 访问会报错，需重新创建或删除                       |\n| **快捷方式（Windows Shortcut）**          | Windows 上的类似软连接概念            | Windows 特有，非 Linux 原生概念              |\n\n#### 5. 总结\n\n* **软连接是文件系统的一种引用方式**，通过路径指向目标文件或目录\n* 与硬连接相比，软连接更灵活，但依赖目标文件的存在\n* 常用于：\n\n  * 配置文件管理（`/etc/config`）\n  * 版本切换（`current` 指向不同版本目录）\n  * 跨目录/跨分区访问\n\n"},"children":[]}]}]},{"data":{"id":"demjnvg1ozs0","created":1764558159071,"text":"场景题","expandState":"expand"},"children":[{"data":{"id":"demjnz2uots0","created":1764558166980,"text":"图书管理系统","expandState":"expand"},"children":[{"data":{"id":"demjo3b1yvk0","created":1764558176184,"text":"一天中我发现上午10点借书的多，下午4点还书的多，怎么解决","expandState":"expand","note":"### 场景题：图书管理系统借还书高峰优化\n\n#### 背景\n- 系统架构：传统 MVC（Model-View-Controller）\n- 观察到：\n  - 上午 10 点借书高峰\n  - 下午 4 点还书高峰\n\n#### 问题\n- 高峰期请求集中，可能导致：\n  - 数据库压力大\n  - 页面响应慢\n  - 用户排队等待时间长\n\n#### 解决方案\n\n1. **借书高峰优化（上午 10 点）**\n   - **异步处理**\n     - 前端提交请求后立即响应，后端通过队列异步处理借书事务\n     - 减少前端等待时间\n   - **队列/缓存**\n     - 使用消息队列（RabbitMQ/Kafka）缓冲请求，平滑处理\n   - **数据库优化**\n     - 批量写入借书记录\n     - 使用索引优化查询\n\n2. **还书高峰优化（下午 4 点）**\n   - **自助归还/扫码**\n     - 推广自助设备，减少人工操作压力\n   - **异步更新**\n     - 后端队列异步处理还书入库，前端立即提示“归还成功”\n   - **流量引导**\n     - 提前提醒用户避开高峰时间\n     - 分散高峰负载\n\n3. **系统架构优化**\n   - **水平扩展 MVC 的 Controller**\n     - 高峰期启动多实例处理请求\n   - **缓存热点数据**\n     - 热门图书库存信息使用缓存减少数据库压力\n\n#### 总结（面试可用一句话）\n通过 **异步处理 + 消息队列 + 数据库优化 + 高峰分流**，可以缓解上午借书和下午还书的高峰压力，提高系统吞吐量和用户体验。\n"},"children":[]},{"data":{"id":"demjom9tde80","created":1764558217467,"text":"某几本书非常火，点击量非常大，怎么设计系统","expandState":"expand","note":"### 场景题：热门图书高点击量系统设计\n\n#### 背景\n- 系统：图书管理系统\n- 问题：部分图书访问量特别高，可能造成数据库压力和性能瓶颈\n\n#### 解决方案\n\n1. **缓存热点数据**\n   - 使用 **Redis/Memcached** 缓存热门图书信息（库存、借阅状态、详情等）\n   - 设置合理过期时间和更新策略（如 TTL 或主动更新）\n   - 减少数据库直接访问压力\n\n2. **异步计数与批量写入**\n   - 点击量或借阅量不需要实时写入数据库\n   - 前端请求直接更新缓存，定期批量同步到数据库\n   - 避免高并发写操作导致 DB 锁\n\n3. **读写分离**\n   - 使用 **主从数据库架构**\n     - 主库写入借阅记录\n     - 从库提供查询服务，支撑大量访问\n   - 热点查询可以直接读从库或缓存\n\n4. **CDN / 静态化处理**\n   - 图书封面、简介等静态资源通过 CDN 提供\n   - 减少应用服务器压力\n\n5. **分布式架构（可选）**\n   - 对于极端热门书籍，考虑：\n     - 独立服务或微服务部署\n     - 分片或分区存储\n\n#### 总结（面试可用一句话）\n通过 **缓存热门数据 + 异步更新 + 读写分离 + CDN 静态化**，系统能够支撑热门图书高访问量，同时保证数据库和服务器性能稳定。\n"},"children":[]},{"data":{"id":"demjovmzhns0","created":1764558237855,"text":"我发现有些时候查询书籍系统响应很慢，是哪里的问题，可以优化什么","expandState":"expand","note":"```markdown\n### 场景题：图书查询响应慢的原因与优化\n\n#### 可能原因\n\n1. **数据库性能瓶颈**\n   - 查询未加索引，导致全表扫描\n   - 热点表访问量大，锁竞争严重\n   - 大量关联查询或复杂 SQL\n\n2. **缓存缺失**\n   - 热门书籍或常用查询未缓存\n   - 每次请求都直接访问数据库\n\n3. **应用层瓶颈**\n   - Controller 或 Service 层处理逻辑复杂\n   - 并发请求过多，线程/连接池耗尽\n\n4. **网络或静态资源**\n   - 大量图片/封面未 CDN 加速\n   - 前端请求阻塞或多次重复请求\n\n---\n\n#### 优化方案\n\n1. **数据库优化**\n   - 为查询字段建立索引（如书名、作者、分类）\n   - 避免不必要的 join 和子查询\n   - 分库分表或读写分离应对高并发\n\n2. **缓存机制**\n   - 热门书籍查询结果放入 Redis/Memcached\n   - 使用 TTL 或主动更新策略\n\n3. **应用层优化**\n   - 逻辑处理异步化或批量化\n   - 连接池合理配置\n   - 使用多线程或协程提升吞吐量\n\n4. **前端和网络优化**\n   - 静态资源（封面、简介）通过 CDN 加速\n   - 前端分页或懒加载，减少一次性查询量\n\n---\n\n#### 总结（面试可用一句话）\n查询响应慢主要可能是 **数据库索引不足、缓存缺失、应用处理逻辑复杂或静态资源阻塞**，通过 **索引优化 + 缓存 + 异步/批量处理 + CDN 静态加速** 可以显著提升响应速度。\n```\n"},"children":[]}]},{"data":{"id":"demjp7e02sg0","created":1764558263433,"text":"我的项目：数据推送任务","expandState":"expand"},"children":[{"data":{"id":"demjpbtx7340","created":1764558273103,"text":"你的数据通过接口成功推送到前端展示之后，你怎么让上级收到通知","expandState":"expand","note":"### 场景题：数据成功推送后上级通知设计\n\n#### 背景\n- 系统流程：\n  1. 从 MySQL 获取数据\n  2. 构建 API 请求\n  3. 服务端处理并推送至前端展示\n- 需求：数据成功展示后，上级收到通知\n\n---\n\n#### 设计方案\n\n1. **前端确认推送状态**\n   - 前端在接收到数据并渲染成功后，向服务端发送确认接口\n   ```text\n   POST /api/acknowledge\n   body: { dataId: 123, status: \"displayed\" }\n````\n\n2. **服务端触发通知**\n\n   * 收到前端确认后，服务端通过以下方式通知上级：\n\n     * **邮件**：SMTP 发送提醒\n     * **企业 IM/消息**：如 Slack、钉钉、微信企业号\n     * **系统消息**：写入消息表，上级前端查看\n\n3. **异步处理通知**\n\n   * 使用消息队列（RabbitMQ/Kafka）将通知任务异步执行\n   * 避免阻塞主业务流程\n   * 可实现重试机制保证通知可靠\n\n4. **通知状态追踪**\n\n   * 在数据库中记录每条数据的推送状态与通知状态\n   * 可统计是否成功通知上级，便于监控\n\n---\n\n#### 总结（面试可用一句话）\n\n数据成功推送前端后，由前端回调确认，服务端异步触发消息队列发送通知（邮件/IM/系统消息），并在数据库记录状态，确保上级可靠收到提醒。\n\n"},"children":[]}]}]},{"data":{"id":"demjq0evihc0","created":1764558326613,"text":"项目描述&项目难点&如何解决","expandState":"expand"},"children":[{"data":{"id":"demjt82kicg0","created":1764558578375,"text":"AI文档校验","expandState":"expand"},"children":[{"data":{"id":"denegm6ez880","created":1764645044756,"text":"前端html你是怎么处理的"},"children":[]},{"data":{"id":"denegtw78z40","created":1764645061552,"text":"你说的这个大大减少token是怎么实现的"},"children":[]}]},{"data":{"id":"demjt924bso0","created":1764558580525,"text":"AI智能问答","expandState":"expand"},"children":[{"data":{"id":"denehcp49p40","created":1764645102483,"text":"所以我理解是你做了一个智能问答系统是吗，这是什么应用场景的"},"children":[]},{"data":{"id":"denehqfgj4w0","created":1764645132374,"text":"sse流式处理和非流式处理有什么区别"},"children":[]}]},{"data":{"id":"demjtao8z740","created":1764558584040,"text":"知识图谱","expandState":"expand"},"children":[{"data":{"id":"denehzhrs800","created":1764645152105,"text":"你这里的知识图谱是怎么用的，你给我讲一下"},"children":[]}]},{"data":{"id":"demknk7ks5s0","created":1764560955724,"text":"agent工作流","expandState":"expand"},"children":[{"data":{"id":"denei7t0lmw0","created":1764645170199,"text":"处理的是什么数据"},"children":[]},{"data":{"id":"denejrkehdc0","created":1764645291578,"text":"多模态图片数据怎么处理的？关键帧是怎么截取到的"},"children":[]},{"data":{"id":"denek0q1pgg0","created":1764645311510,"text":"关键帧截取用的什么方法，什么模型，逻辑是什么"},"children":[]}]},{"data":{"id":"demknqqyzjs0","created":1764560969958,"text":"高校成果转换平台","expandState":"expand"},"children":[{"data":{"id":"deneicni2aw0","created":1764645180750,"text":"描述一下这个项目"},"children":[]},{"data":{"id":"deneigmrns00","created":1764645189412,"text":"你有提到加密，你的明文加密怎么做的"},"children":[]},{"data":{"id":"deneimbwlf40","created":1764645201816,"text":"我还想知道你这个里面登陆的逻辑是什么"},"children":[]},{"data":{"id":"deneiw38grk0","created":1764645223060,"text":"cookie或者session是怎么设置的"},"children":[]}]},{"data":{"id":"demknx1q7zs0","created":1764560983669,"text":"就业智能体","expandState":"expand"},"children":[{"data":{"id":"demnpx078zc0","created":1764569603635,"text":"我看里面有数字人你讲一下具体有哪些模块，用的什么模型怎么实现的"},"children":[]},{"data":{"id":"deneumg8hoo0","created":1764646142448,"text":"基座模型用的什么"},"children":[]}]}]},{"data":{"id":"demjr7om0hk0","created":1764558420803,"text":"前端","expandState":"expand","layout_mind_offset":{"x":28.749999571591616,"y":22.499999664723873}},"children":[{"data":{"id":"demjrahfv400","created":1764558426900,"text":"你对跨域问题有什么了解","expandState":"expand"},"children":[]},{"data":{"id":"demjrhhn3r40","created":1764558442149,"text":"关于代理你了解多少","expandState":"expand"},"children":[]},{"data":{"id":"demjse2medk0","created":1764558513075,"text":"单点登录有做过吗","expandState":"expand"},"children":[]},{"data":{"id":"demjslaj7xs0","created":1764558528791,"text":"后端怎么和前端对接","expandState":"expand"},"children":[]},{"data":{"id":"demjssvnmf40","created":1764558545305,"text":"前端怎么做性能优化","expandState":"expand"},"children":[]},{"data":{"id":"demko9jgudc0","created":1764561010863,"text":"图片压缩你做过吗","expandState":"expand"},"children":[]},{"data":{"id":"denejdesh1k0","created":1764645260764,"text":"cookie的原理是什么？有哪些字段？和session的区别是什么？"},"children":[]},{"data":{"id":"deo7w5hf9ts0","created":1764728074430,"text":"http和https的区别"},"children":[]},{"data":{"id":"deo7wa5ua7s0","created":1764728084614,"text":"对称加密和非对称加密的区别"},"children":[]}]},{"data":{"id":"demjukxvgio0","created":1764558684754,"text":"agent相关","layout_mind_offset":{"x":-9,"y":2},"expandState":"expand"},"children":[{"data":{"id":"demk6c6p4880","created":1764559606066,"text":"精调参数有哪些，你怎么设置的，为什么","expandState":"expand","note":"### 场景题：训练精参数说明与解释\n\n#### 背景\n- 基础模型：`Doubao-Seed-1.6-flash`\n- 训练方式：SFT 全量训练\n- 数据集总 Tokens：216,710,212\n- 精参数配置影响模型收敛速度、性能和训练稳定性\n\n---\n\n#### 1. 精参数及含义与作用\n\n| 参数 | 值 | 作用 | 解释 |\n|------|-----|------|------|\n| `batch_size` | 8 | 每次训练的样本数量 | 每批输入模型的数据量，影响显存占用和梯度稳定性；batch 越大，梯度越稳定，但显存需求增加 |\n| `epoch` | 2 | 全量训练循环次数 | 遍历整个训练数据集的次数；控制训练轮数，过多容易过拟合，过少可能欠拟合 |\n| `learning_rate` | 0.00001 | 学习率 | 控制权重更新幅度；SFT 微调使用小 lr 避免破坏预训练知识 |\n| `seq_len` | 32768 | 最大输入序列长度 | 模型处理的最大 token 数；保证长文本不被截断，适合大上下文任务 |\n| `freeze_llm` | false | 是否冻结 LLM 模型权重 | false 表示全量微调；true 表示冻结预训练模型，只训练头部任务层 |\n| `freeze_vit` | false | 是否冻结视觉编码器 | 任务中无视觉输入时可保持 false；如冻结则减少训练量和显存消耗 |\n| `warmup_step_rate` | 0.05 | 学习率预热比例 | 训练初期缓慢增加学习率，避免梯度震荡，稳定收敛 |\n| `最大保存模型数量` | 10 | 控制磁盘占用 | 最多保留 10 个 checkpoint，便于回滚和调参 |\n| `保存产物间隔` | 0.2 epoch | 模型保存频率 | 每 0.2 个 epoch 保存一次模型，间隔过小则过于频繁，过大可能丢失中间状态 |\n| `加密密钥` | 平台托管密钥 | 模型和数据安全 | 确保训练数据和模型产物加密，符合企业安全策略 |\n\n---\n\n#### 2. 参数设置 rationale\n\n1. **batch_size & seq_len**  \n   - 小 batch 避免显存超限，同时保证梯度稳定\n   - seq_len 大保证长文本输入完整，不丢信息\n\n2. **epoch & learning_rate**  \n   - 2 个 epoch 足够 SFT 微调收敛\n   - 超小 learning_rate 避免破坏预训练知识\n\n3. **freeze_llm / freeze_vit**  \n   - 不冻结允许全量微调，提高模型任务适应性\n   - 可根据资源和任务复杂度选择冻结\n\n4. **warmup_step_rate**  \n   - 平滑升学习率，防止初期梯度过大导致不稳定\n\n5. **保存策略 & 加密**  \n   - 保证训练过程可追溯\n   - 数据与模型安全符合平台要求\n\n---\n\n#### 总结（面试可用一句话）\n每个精参数都有明确含义：**batch_size/seq_len 决定数据输入和显存占用，epoch/learning_rate 控制收敛，freeze 决定微调范围，warmup 防止梯度震荡，保存与加密保障安全与可追溯性**，我根据任务和硬件资源合理配置，保证 SFT 微调稳定、高效。\n"},"children":[]},{"data":{"id":"demk8cdw2t40","created":1764559763229,"text":"精调的结果你怎么判断，有哪些指标，结果怎么样","expandState":"expand","note":"### 场景题：精调结果判断与指标分析\n\n#### 1. 关键指标\n\n| 指标 | 含义 | 观察与分析 |\n|------|------|------------|\n| `train/loss` | 训练损失 | 损失快速下降至接近 0，说明模型在训练集上拟合良好 |\n| `eval/loss` | 评估损失 | 与训练损失接近且稳定，表明模型在验证集上泛化正常 |\n| `num_tokens` | 输入 token 总数 | 波动稳定，无异常，数据加载正常 |\n| `num_samples` | 样本数 | 稳定，训练数据量正常 |\n| `min_tokens` / `max_tokens` | 单样本 token 范围 | 长度分布一致，未出现截断或异常 |\n| `lr` | 学习率 | 初期快速下降并逐渐稳定，符合预热 + 衰减策略 |\n| `grad_norm` | 梯度范数 | 初期较大后收敛至接近 0，参数更新稳定 |\n| `avg_loss_weight` | 损失权重均值 | 稳定接近 1，说明损失配置正常 |\n\n---\n\n#### 2. 判断模型训练结果的方法\n\n1. **损失收敛性**\n   - 训练损失和评估损失持续下降并趋于平稳\n   - 训练过程中无震荡或发散现象\n   - **结论**：模型拟合稳定，未欠拟合或过拟合\n\n2. **数据指标稳定性**\n   - token 总数、样本数、单样本长度保持稳定\n   - **结论**：数据输入正常，训练流程可靠\n\n3. **训练参数指标**\n   - 学习率按策略衰减，梯度范数收敛\n   - **结论**：训练后期参数更新稳定，梯度安全\n\n4. **辅助验证**\n   - 可在训练结束后查看任务相关指标（如准确率、召回率）\n   - 对比训练损失与评估损失差异，确保泛化能力\n\n---\n\n#### 3. 结果总结（面试可用一句话）\n\n本次精调训练过程 **稳定健康**：损失快速收敛且无波动，数据和参数指标均正常，说明模型在当前任务上拟合良好，后续可通过最终评估指标验证效果。\n"},"children":[]},{"data":{"id":"demk65d8qxs0","created":1764559591224,"text":"精调保存的多个模型选哪一个最优","expandState":"expand","note":"### 场景题：精调模型选择与多模型来源\n\n#### 1. 多个模型的来源\n- **训练过程中周期性保存**：\n  - 根据训练配置 `保存产物间隔`（如每 0.2 个 epoch 保存一次模型）\n  - 每次保存称为一个 checkpoint\n- **原因**：\n  - 训练可能中断或出现异常，可回滚到最近 checkpoint\n  - 便于后续对比不同阶段模型效果\n- **最终会产生**：\n  - 训练早期的模型（loss 高）\n  - 中期模型（loss 下降趋势）\n  - 后期模型（loss 收敛稳定）\n\n---\n\n#### 2. 如何选择最优模型\n\n1. **评估指标选择**\n   - 使用验证集（eval set）计算关键指标：\n     - 损失（`eval/loss`）\n     - 任务相关指标（如准确率、召回率、F1、BLEU、ROUGE 等）\n   - 选择在验证集上**损失最小或任务指标最优的 checkpoint**\n\n2. **稳定性考虑**\n   - 观察训练曲线，避免选择刚刚收敛但可能波动的 checkpoint\n   - 对比相邻多个 checkpoint 指标，选择最稳定且指标最优者\n\n3. **实际应用测试**\n   - 可在小规模线上或离线数据上进行推理测试\n   - 检查实际效果是否符合预期，防止过拟合\n\n---\n\n#### 3. 总结（面试可用一句话）\n训练过程中周期性保存的多个 checkpoint 提供模型回滚和对比的机会，通过 **验证集指标 + 稳定性分析 + 实际推理测试** 选择损失最小或任务指标最优的模型作为最终精调产物。\n"},"children":[]},{"data":{"id":"denlyzv3pi80","created":1764666232871,"text":"在 Python 多进程 + LLM API 批处理百万级 Parquet 数据时，即使优化代码并发，吞吐量仍受限，任务速度不稳定。","note":"## 面试回答：任务执行命令优化为何能从 3/s 提升到 50/s？\n\n### 背景\n\n原来运行命令：\n\n```bash\nmlx worker quota\n\n/usr/bin/python3 /mlx/users/liuyuting.1020/sft_test.py\n```\n\n处理速度：**20/s → 3/s → 卡死**\n\n优化后运行命令：\n\n```bash\nlaunch --cpu=100 --gpu=0 --memory=97 /usr/bin/python3 /mlx/users/liuyuting.1020/sft_test.py\n```\n\n处理速度：**持续稳定 50/s**\n\n---\n\n## 核心原理（面试官爱听）\n\n### 原因本质：**使用了专属 Worker 资源，而不是共享环境抢资源**\n\n| 维度     | 原命令             | launch 命令       |\n| ------ | --------------- | --------------- |\n| CPU 资源 | 公共池抢占，共享，易饱和    | 独享 **100 core** |\n| 内存     | 不保证 → 容易 OOM 回收 | 独享 **97GB**，无抢占 |\n| 任务调度   | 动态抢资源 → 波动大     | 独占模式：稳定、无降速     |\n| 操作系统行为 | 被系统 throttle/限制 | 完全按资源配额执行       |\n\n> ⚠共享环境下：LLM API 并发 + 网络 + 多进程会让系统对你限流 → **反压导致卡死**\n\n---\n\n## 为什么性能稳定提升？\n\n* CPU 从 *“分配多少吃多少”* → *“固定分配，保证吞吐”*\n* 内存充足，不会被系统 OOM-killer 暗杀\n* OS 不再动态限速（cgroups/namespace 限制解除）\n* Worker 绑定专用 NUMA Node，减少 **上下文切换**\n* API 请求排队减少 → 网络延迟显著回落\n\n---\n\n## 面试可总结一句话\n\n> **原命令是“拼桌吃饭”，资源抢不过别人就越来越慢**\n> **launch 是“包间独享”，提前锁定资源，吞吐稳定不掉速**\n\n\n---\n\n## 最佳实践建议（顺便表现你有经验）\n\n```bash\n# 推荐固定资源策略\nlaunch --cpu=100 --memory=97 --gpu=0 \\\n   /usr/bin/python3 sft_test.py --workers=30 --batch-size=500\n```\n\n---\n\n## 一句话总结\n\n> 并发处理任务要 **算力“确定性” > 峰值速度**\n> 用 launch 保证资源隔离，避免系统隐性限流和资源抢占，是批处理性能稳定的关键。\n\n\n"},"children":[]},{"data":{"id":"demnf2ce3m80","created":1764568753250,"text":"讲一下a2a协议","expandState":"expand","note":"### A2A：Agent-to-Agent 协议（直切要点）\n\nA2A 在智能体领域指 **Agent-to-Agent** 通信协议，用于不同智能体之间的**自治协作、消息交换、任务分工、状态共享**。\n\n核心目标：  \n让多个智能体像微服务一样**互操作、互协同、互验证**，形成多智能体系统。\n\n---\n\n### 本质\nA2A 的核心是 **标准化消息通信协议**：\n不同 Agent 不需要知道对方实现，只依赖**统一的通信语义**。\n\n---\n\n### A2A 标准语义框架（通用结构）\n常见采用 **FIPA-ACL** 或其演化设计：\n\n| 字段 | 含义 |\n|------|------|\n| performative | 行为意图，如 request、inform、agree、refuse |\n| content | 要执行的任务或数据 |\n| language | 内容编码（LLM prompt、JSON 等） |\n| ontology | 任务领域本体（共享语义标准） |\n| protocol | 对话流程，如协商/竞价 |\n| sender/receiver | Agent 标识与寻址 |\n\n这种协议定义了**如何说**与**说什么**。\n\n---\n\n### A2A 的通信方式\n1. **直接调用**：HTTP/gRPC（同步）\n2. **消息总线**：MQ/Kafka（异步解耦）\n3. **事件驱动**：发布-订阅 EventBus\n4. **语义协作**：通过 LLM 解释 ACL\n\n选择取决于一致性与时延要求。\n\n---\n\n### 能力增强点（面试重点）\n为了实现真正自治，A2A 协议需要支持：\n\n| 能力 | 技术机制 |\n|------|------|\n| 协商与分工 | Contract-Net Protocol、竞价、优先级 |\n| 可信传输 | 签名验证、token 权限 |\n| 幂等保障 | 消息去重、事务补偿 |\n| 状态可观测 | TraceID、心跳检测 |\n| 演化扩展 | 新 agent 插拔式加入 |\n\n体现为**去中心化的协作能力**。\n\n---\n\n### 和单 Agent 架构区别\n| 单 Agent | 多 Agent（A2A） |\n|---------|----------------|\n| 一体化执行 | 任务拆解/分布式协作 |\n| 资源瓶颈明显 | 异构能力互补 |\n| 易维护 | 复杂度上升（协议与调度） |\n\n一句话：  \nA2A 让智能体系统从“单体”迈向“自治协同”。\n\n---\n\n### 20 秒面试回答模板\n> A2A 指的是 Agent-to-Agent 协议，用于多智能体之间标准化通信和协同。  \n> 它通常基于 FIPA-ACL 的语义结构，例如定义 performative、content、ontology 等字段，让不同 Agent 能互相理解与协作。  \n> 通过事件驱动或消息总线实现可靠传输，同时支持协商、幂等、安全与可观察性，确保自治智能体能稳定协作。\n\n---\n\n如果你需要，我可以继续：\n1. 给你画一个多 Agent 协作架构图\n2. 结合你的 LLM 海量 API 并发经验写成多 Agent 场景话术\n3. 给你提供一个 A2A 消息模板和最简参考实现（Python/Golang）  \n要不要我帮你整理成面试简历亮点？\n\n### A2A 协议概念（开门见山）\nA2A（Application-to-Application）指**系统之间通过自动化接口进行直接通信的协议或机制**。  \n核心目标：让两个服务端系统**无人工干预地相互调用、数据流转、业务联动**。\n\n它不是某一个固定协议，而是一类**系统级对接模式**，本质是**服务间接口标准**。\n\n---\n\n### 为什么存在 A2A\n传统人工或人工+Web 的流程效率低、易错，因此企业系统（ERP/CRM/Billing/Trading/支付等）需要：\n- 数据实时同步\n- 业务流程跨系统触发\n- 批量、高频、长链路调用\n\n→ 就出现了 A2A 这种标准化系统对系统集成方式。\n\n---\n\n### A2A 与 B2B/B2C 区别\n| 类型 | 全称 | 交互主体 | 场景 | 接口特征 |\n|------|------|---------|------|------|\n| A2A | Application-to-Application | 企业内部系统之间 | 企业内部流程打通 | 高实时性、高频、自动化 |\n| B2B | Business-to-Business | 企业之间 | 订单/结算对接 | 更注重安全、审计、异步 |\n| B2C | Business-to-Consumer | 企业与用户 | 客户操作 | Web/移动 API |\n\n一句话：  \n**A2A偏系统内部实时协同，B2B偏企业间数据交换。**\n\n---\n\n### A2A 常用协议形态\nA2A 不是单一协议，通常采用以下技术承载：\n\n1. REST API（HTTP/JSON）\n2. gRPC（高性能 RPC）\n3. SOAP（老式金融/政企系统仍普遍）\n4. 消息中间件（异步 MQ）\n5. ESB / EAI 中间件（企业集成总线）\n6. WebSocket/事件总线（实时变更推送）\n\n可总结为两大方向：\n- **同步直连**：低延迟，高耦合（REST/gRPC）\n- **异步解耦**：高吞吐，高容错（Kafka/RabbitMQ）\n\n---\n\n### 关键要点（面试认可）\n1. 安全认证：OAuth2.0 / MTLS / HMAC / IP 白名单  \n2. 可观察性：Tracing + Metrics + Logging  \n3. 异常回滚：幂等、补偿事务（Saga/TCC）  \n4. 负载与限流：重试/退避/熔断/隔离  \n5. 数据一致性：最终一致性为主  \n6. 版本兼容：接口字段升级策略\n\n这些是系统对接必考点。\n\n---\n\n### 面试简答模板（20 秒答法）\n> A2A 是 Application-to-Application 的系统对系统集成协议形态。  \n> 用于企业内部多个服务之间自动化数据交换与流程协同。  \n> 一般通过 REST/gRPC/MQ 等技术实现，要求安全认证、幂等、限流与可观察性保障可靠性。  \n> 它区别于 B2B：A2A更偏内部实时联动作业，高频高并发。\n\n---\n\n如果你需要，我可以：\n- 给你画一张 **A2A 架构示意图**\n- 补充 **支付/电商/金融业务案例**\n- 给出 **面试代码与稳定性策略回答**\n\n要我按你最近的多进程+API任务背景改写成你的简历和面试话术吗？\n"},"children":[]},{"data":{"id":"demk8kaqtvc0","created":1764559780453,"text":"你搭建多agent工作流的整体步骤是什么","expandState":"expand","note":"### 多 Agent 工作流搭建的完整步骤（包含数据流设计）\n\n#### 1）任务分解与 Agent 责任边界设计\n- 明确任务目标\n- 拆分子任务，对每个 Agent 定义：\n  - 能力范围\n  - 输入/输出类型\n  - 验证规则与失败回退机制\n\n#### 2）数据流与上下文流转设计（新增并强调）\n- 定义数据如何从 Agent A 流向 B\n- 数据携带哪些上下文，是否需要裁剪或增强\n- 是否采用：\n  - 单向流水线数据流\n  - DAG 依赖图数据流\n  - 黑板模型共享数据流\n- 设计中间缓存与状态持久化\n\n一句话：规划**数据怎么流、状态如何保持**。\n\n#### 3）Agent 间通信协议（A2A）定义\n- 统一消息格式（通常 JSON）\n- 状态码与任务回执机制\n- 会话管理：上下文 ID、对话轮数、协作标记\n- 支持同步/异步通信模式\n\n一句话：规划**怎么沟通、怎么确认成功**。\n\n#### 4）Orchestrator/调度器流程编排\n- 管控 Agent 协作顺序与重试逻辑\n- 动态选择最合适 Agent（如评分、性能策略）\n- 管理依赖 DAG 与全局状态机\n\n一句话：安排**谁何时做、失败怎么办**。\n\n#### 5）Agent 能力实现与工具挂载\n- 模型选择（LLM、规则引擎、工具调用）\n- 跨系统集能力如 DB、API、文件处理\n- 扩展插件系统供后期增强能力\n\n一句话：给 Agent **赋能**。\n\n#### 6）记忆管理与结果融合\n- 本地短期记忆\n- 全局黑板长期记忆\n- 多结果决策融合（投票、排序、裁决）\n- 面向用户或 API 的统一输出\n\n一句话：保证**历史可追溯，结果可复用**。\n\n#### 7）监控、评估与动态优化\n- 吞吐量、成功率、延迟监控\n- 热点 Agent 负载均衡\n- 自动扩容与限流策略\n- 在线评估与实时 Prompt 优化\n\n一句话：让系统**越跑越好**。\n\n---\n\n### 面试总结一句话\n> 多 Agent 工作流搭建遵循**任务分解 → 数据流设计 → 通信协议 → 流程编排 → Agent 能力实现 → 记忆与融合 → 监控优化**七步走，\n> 核心是让数据正确流动、让 Agent 高效协同、让系统可持续演进。\n\n---\n\n如果你愿意，我可以把这个：\n- 变成**你的项目案例版**（百万数据 LLM 多 Agent 协作）\n- 加**架构图 + 性能提升数据 + 面试答题节奏**\n\n需要我直接整合进你的面经吗？\n"},"children":[]},{"data":{"id":"demk5kk81xk0","created":1764559545934,"text":"你怎么优化agent的效果","expandState":"expand","note":"### 面试题：你怎么优化 Agent 的效果？\n\n#### 核心思路\n从三个维度优化 Agent：\n1）让 Agent 本身更强  \n2）让 Agent 协作更高效  \n3）让不确定的模型行为更可控\n\n---\n\n### 1）Prompt 优化（行为明确化）\n- 明确角色、能力、边界\n- 输入结构化（JSON/Schema）\n- 输出强约束（格式校验、防幻觉）\n- 提供成功案例和反例\n\n效果：减少误判与幻觉，提升稳定性。\n\n---\n\n### 2）工具增强（能力扩展）\n- 接入外部工具：DB、API、检索系统\n- 对模型输出进行二次验证，如：\n  - 字段校验\n  - 规则检查\n  - 模型自评与互评交叉\n\n效果：让 Agent 不再只依赖 LLM 的“猜”。\n\n---\n\n### 3）任务链路优化（更聪明的协作）\n- 合并无效中间 Agent，减少链路长度\n- DAG 调度避免串行瓶颈\n- 为关键 Agent 增加 fallback 机制：\n  - 超时自动切备用策略\n  - 失败重试与降级输出\n\n效果：整体吞吐量和成功率提升。\n\n---\n\n### 4）记忆管理（减少信息遗漏）\n- 短期记忆：当前任务上下文缓存\n- 长期记忆：全局知识库+历史记录\n- 历史裁剪策略：仅保留关键依赖\n\n效果：减少重复推理、错误累积。\n\n---\n\n### 5）反馈学习（闭环优化）\n- 用户反馈→自动修正决策逻辑\n- 高置信结果加入再训练/强化样本\n- 行为数据用于 prompt 演进与模型微调\n\n效果：Agent 越用越准。\n\n---\n\n### 面试一句总结\n> 我从 Prompt、工具能力、协作链路、记忆管理和反馈学习五个方向优化 Agents，实现更高的准确率与吞吐量，让系统具备持续演化能力。\n"},"children":[]},{"data":{"id":"demjv1du5rs0","created":1764558720547,"text":"mcp了解吗","expandState":"expand","note":"了解的，这里的 **MCP** 是指 **Model Context Protocol**（大模型上下文协议），不是微软那个或别的缩写。\n\n---\n\n## MCP 是什么？\n\nMCP 是一种 **标准化的多智能体 / 工具调用协议**\n→ 让 **模型可以安全、统一、结构化地访问外部资源**（数据库、API、本地工具等）\n\n简单说：\n\n> 给模型一个“统一的外部世界接口标准”，不再靠杂乱的 prompt hack\n\n---\n\n## 在 MCP 出现之前，我们通常怎么做？\n\n| 阶段                                  | 使用方式                     | 存在的问题           |\n| ----------------------------------- | ------------------------ | --------------- |\n| **1. Prompt Hack**                  | 纯靠提示词让模型“假装”能访问东西        | 不稳定、不可控、易幻觉     |\n| **2. Function Calling（OpenAI）**     | API 调用函数，由模型选择参数         | 仅限自定义函数，扩展性有限   |\n| **3. Tools / Plugins（LangChain 等）** | 绑定外部 API、数据库、浏览器工具       | 生态碎片化、互不兼容、安装复杂 |\n| **4. Multi-Agent Framework**        | 多智能体协调任务                 | 协议不统一，通信混乱      |\n| **5. MCP（最新阶段）**                    | **统一协议 + 权限管理 + 实时资源接入** | ——              |\n\n---\n\n## 为什么 MCP 更先进？\n\n| 能力    | 之前方式       | MCP           |\n| ----- | ---------- | ------------- |\n| 接外部资源 | 复杂 / 各框架不同 | **标准化工具接口**   |\n| 权限安全  | 基本无        | **用户授信、可控权限** |\n| 实时性   | 差          | **可主动推送（流式）** |\n| 模型协同  | 手写协议       | **统一资源调用协议**  |\n| 开发成本  | 高          | 低、可跨模型通用      |\n\n一句总结：\n\n> MCP 统一了 **工具访问方式 + 多AI通信机制 + 用户授权体系**\n\n---\n\n## 多智能体（Agent-to-Agent）在 MCP 中如何通信？\n\n* 每个 Agent 都能被视为一个 **MCP Server**\n* 另外的 Agent 可以作为 **Client** 调用它\n* 资源都是 **标准化 JSON Schema**\n\n很像微服务结构\n→ Agent = Service\n→ 通过 MCP 协议互调\n\n---\n\n## 什么时候会重点提 MCP？\n\n面试官提到以下关键词都能切入：\n\n* 工具调用（Tools）\n* 企业内部知识库接入\n* 多智能体协同工作流\n* 权限安全的数据读写\n* 统一 API 接口层\n\n可以一句话展示能力：\n\n> 以前工具调用和 Agent 协同缺乏统一标准，MCP 给到了统一协议和权限治理能力，更适合生产落地。\n\n---\n\n## 面试押题式总结回答\n\n> 在 MCP 出来之前，主要是 Function Calling + 多工具框架（如 LangChain）实现扩展，但各家协议不统一，权限风险高。MCP 提供了统一的工具 & 资源访问协议、多智能体通信标准，并加入用户授权与安全控制，让模型真正具备安全、可控地访问外部系统的能力，是大模型生产化的重要基建。\n"},"children":[]},{"data":{"id":"demk8tz0f740","created":1764559801512,"text":"agent架构里面具体有哪些东西，分别有什么用","expandState":"expand","note":"# Agent 架构核心组成与作用\n\n## 1. Agent Core（决策层）\n- 包含大模型或策略模型\n- 负责理解指令、制定行动计划、决策下一步调用的工具或发给其它 Agent 的任务\n\n## 2. Memory（记忆模块）\n- 存储长期知识、短期上下文历史\n- 减少重复推理、提高一致性\n\n类型：\n- Long-term Memory：知识库、经验总结\n- Short-term / Working Memory：对话上下文、近期任务状态\n\n## 3. Tools / Skills（工具能力层）\n- 外部能力访问接口，如检索、API 调用、数据库、程序执行\n- 补充模型在事实、计算、实时信息方面的能力\n\n## 4. Planner（任务规划模块）\n- 将复杂目标分解成可执行子任务\n- 决定任务执行顺序，减少盲目试错\n\n## 5. Executor（执行模块）\n- 具体执行工具调用或操作步骤\n- 负责收集执行结果回传给 Core\n\n## 6. Communication / Protocol（通信模块）\n- Agent 之间或与前端/服务端之间的消息协议\n- 支持协作、多 Agent 调度（如 MCP / OpenAI ReAct 等）\n\n## 7. State Manager（状态管理）\n- 维护任务进行中状态、防止重复执行\n- 任务失败时支持回滚或重试\n\n---\n\n# 总结一句话\nAgent 架构 = 决策（Core） + 记忆（Memory） + 执行（Tools/Executor） + 协同（Protocol） + 状态管理（State）\n\n分层目的：\n- 可控决策\n- 可持续记忆\n- 可扩展能力\n- 可协作工作流\n- 可监控执行过程\n"},"children":[]},{"data":{"id":"demnomuabu00","created":1764569503145,"text":"写调用大模型的脚本要设置哪些字段","note":"# 调用大模型脚本核心字段\n\n在 Python 中调用 LLM（如 OpenAI GPT 或自定义 SDK），通常需要设置以下字段：\n\n---\n\n## 1. 模型选择\n- `model`：指定要调用的模型名称\n  - 示例：`\"gpt-4\"`, `\"Doubao-Seed-1.6-flash\"`\n\n## 2. 输入内容\n- `prompt` / `messages`：任务或对话输入\n  - 单轮文本：`prompt=\"...\"`  \n  - 多轮对话：`messages=[{\"role\":\"user\",\"content\":\"...\"}]`\n\n## 3. 输出控制\n- `max_tokens`：生成文本的最大 token 数\n- `temperature`：随机性控制，越低越确定（0-1）\n- `top_p`：核采样概率阈值（0-1）\n- `stop`：结束生成的标记\n\n## 4. 批量与并发\n- `batch_size`：一次性输入的样本数量\n- 并发/多线程参数：如 `max_workers` 或自定义线程池\n\n## 5. API 相关\n- `api_key`：认证秘钥\n- `timeout`：超时时间\n- `retries`：失败重试次数\n\n## 6. 高级配置（可选）\n- `logprobs`：获取 token 概率\n- `presence_penalty` / `frequency_penalty`：控制重复\n- `user`：标记用户身份，便于审计\n- `model_kwargs`：自定义 SDK 特定参数\n\n---\n\n# 简单示例（OpenAI Python SDK）\n```python\nimport openai\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\"role\":\"user\",\"content\":\"请总结以下文本\"}],\n    max_tokens=512,\n    temperature=0.3,\n    top_p=0.9,\n    stop=None,\n)\n"},"children":[]},{"data":{"id":"deod4jenxyw0","created":1764742837201,"text":"讲一下rag的工作原理吧","note":"# RAG（Retrieval-Augmented Generation）工作原理\n\nRAG 是一种将 **信息检索（Retrieval）** 与 **生成式模型（Generation）** 相结合的技术，用于增强生成模型对知识的记忆与回答能力。\n\n\n## 1. 核心思想\n> 在生成答案之前，先检索相关知识文档，然后将这些文档作为上下文提供给生成模型，提升回答准确性与覆盖面。\n\n\n## 2. 流程步骤\n\n### Step 1：查询输入\n- 用户提供问题或指令 `query`\n- 生成向量表示（embedding）\n\n### Step 2：知识检索（Retriever）\n- 在知识库或文档集合中检索最相关的条目\n- 方法：\n  - 向量相似度检索（Faiss、Milvus）\n  - 关键字搜索（Elasticsearch、SQL）\n- 输出 top-k 文档（`context_documents`）\n\n### Step 3：上下文拼接\n- 将检索到的文档与用户问题拼接成生成模型输入\n- 可采用模板：\n```\nQuestion: {query}\nContext: {retrieved_docs}\nAnswer:\n```\n\n\n### Step 4：生成答案（Generator）\n- 将拼接后的文本输入 LLM\n- 模型根据检索到的知识生成回答\n- 可控制：\n  - `max_tokens`\n  - `temperature`\n  - `top_p`\n\n### Step 5：可选反馈/循环\n- 对生成结果进行验证、评分\n- 失败可触发二次检索或更换上下文\n- 支持多轮交互和自增强\n\n\n## 3. 优势\n- **知识增强**：可回答训练数据外的问题\n- **可扩展性**：知识库可动态更新\n- **准确性高**：减少模型凭空生成（幻觉）\n\n\n## 4. 架构示意（简化）\n\nUser Query\n↓\nEmbed Query\n↓\nRetriever (Vector DB / Search)\n↓\nTop-K Documents\n↓\nContext + Query\n↓\nGenerator (LLM)\n↓\nAnswer Output\n\n\n## 5. 面试总结一句话\n> RAG 通过“先检索再生成”的方式，将外部知识注入到生成模型输入中，从而提升\n"},"children":[]},{"data":{"id":"deod4okpu4w0","created":1764742848451,"text":"rag如何向量化，用的是那种数据库","note":"# RAG 向量化与存储数据库\n\n在 RAG（Retrieval-Augmented Generation）中，核心是 **将文本或文档向量化，并快速检索**。\n\n\n## 1. 文本向量化（Embedding）\n- **目的**：把自然语言文本转为固定维度向量，便于计算相似度\n- **常用方法**：\n  - OpenAI Embedding API：`text-embedding-3-small`、`text-embedding-3-large`\n  - Sentence Transformers / BERT / MiniLM\n  - 自研模型（针对特定领域微调的 embedding 模型）\n- **流程**：\n  1. 文档分段（chunking）\n  2. 每段文本生成 embedding\n  3. 保存向量用于相似度检索\n\n\n## 2. 向量数据库（Vector DB）\n- **用途**：存储向量、快速近邻检索\n- **常见数据库**：\n  | 数据库 | 特点 |\n  |--------|------|\n  | FAISS  | Facebook 开源，CPU/GPU 支持，内存检索快 |\n  | Milvus | 高性能向量库，支持分布式、GPU |\n  | Pinecone | 云向量 DB，提供 API、托管服务 |\n  | Weaviate | 支持向量 + 元数据联合查询 |\n  | ElasticSearch + kNN | 可以在 ES 中做向量搜索 |\n\n\n## 3. 检索流程（向量化结合 DB）\n1. 用户输入问题 → 生成 embedding\n2. 查询向量数据库 → 返回 top-k 最近向量\n3. 将对应文档作为生成模型上下文\n\n\n## 4. 典型实现\n```python\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n\n# 文档向量化\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\ndoc_embeddings = model.encode(doc_texts)\n\n# 建立 FAISS 索引\nindex = faiss.IndexFlatL2(doc_embeddings.shape[1])\nindex.add(np.array(doc_embeddings))\n\n# 查询\nquery_vec = model.encode([\"用户问题\"])\nD, I = index.search(np.array(query_vec), k=5)\nretrieved_docs = [doc_texts[i] for i in I[0]]\n````\n\n\n## 5. 面试总结一句话\n\n> RAG 通过文本向量化将自然语言映射到向量空间，再用向量数据库（FAISS、Milvus、Pinecone 等）进行近邻检索，将最相关文档作为 LLM 输入上下文，从而增强生成效果。\n"},"children":[]}]},{"data":{"id":"demjuplw8gw0","created":1764558694913,"text":"大模型算法","expandState":"expand"},"children":[{"data":{"id":"demkb3x4z480","created":1764559979895,"text":"描述一下transformer架构","expandState":"expand","note":"# Transformer 架构简述\n\nTransformer 是一种基于 **自注意力机制（Self-Attention）** 的序列建模架构，用于替代 RNN/LSTM，实现更强的并行性和更长距离的依赖建模。\nhttps://zhuanlan.zhihu.com/p/338817680\n---\n\n## 1. 整体结构\nTransformer 由 **Encoder** 和 **Decoder** 两部分组成：\n\n- Encoder：N 个堆叠的编码层\n- Decoder：N 个堆叠的解码层（多了 Masked Attention 和跨注意力）\n\n主干结构完全依赖注意力机制，不再使用循环或卷积。\n\n---\n\n## 2. Encoder（编码器）\n每一层包括：\n\n1. **Multi-Head Self-Attention（多头自注意力）**\n   - 计算序列内部 token 之间的相关性\n   - 公式核心是 Q、K、V 的点积注意力\n\n2. **Feed Forward Network（前馈全连接网络）**\n   - 两层 MLP，对每个 token 独立处理\n\n3. **LayerNorm + Residual（层归一化 + 残差）**\n   - 保持梯度稳定\n\n**输入 = token embedding + position embedding**\n\n---\n\n## 3. Decoder（解码器）\n相比 Encoder 多两个功能：\n\n1. **Masked Self-Attention**\n   - 阻止模型看到未来 token（因自回归生成）\n\n2. **Cross-Attention（跨注意力）**\n   - Query 来自 decoder\n   - Key/Value 来自 encoder\n   - 将 Encoder 语义注入 Decoder\n\n3. **Feed Forward Network**\n4. **LayerNorm + Residual**\n\n---\n\n## 4. Self-Attention 的核心\n每个 token 通过 Q/K/V 与所有 token 做加权汇总：\n\n```\n\nattention(Q,K,V) = softmax(QKᵀ / sqrt(d_k)) · V\n\n```\n\n作用：\n- 捕获全局依赖\n- 并行计算（避免 RNN 的序列依赖）\n- 多头机制让模型关注不同语义子空间\n\n---\n\n## 5. Transformer 的优势\n- 全序列并行，训练速度快\n- 长距离依赖建模能力强\n- 擅长大规模训练（LLM 基础架构）\n- 可以堆叠更深层数\n\n---\n\n## 6. 面试总结一句话\n> Transformer 由 Encoder 和 Decoder 堆叠构成，以多头自注意力为核心，通过 Q/K/V 计算全局依赖，彻底摆脱了循环结构，是现代大模型（GPT/BERT）的基础架构。\n"},"children":[]},{"data":{"id":"demkbinnwu80","created":1764560011973,"text":"讲一下encoderonly和decoderonly，并举例","expandState":"expand","note":"# Encoder-only 与 Decoder-only 模型简述（带示例）\n\n## 1. Encoder-only（只用 Encoder）\n### 原理\n仅使用 Transformer 的 **Encoder 堆叠结构**，依赖 **Bidirectional Self-Attention（双向注意力）**。\n模型在处理每个 token 时可以看到 **上下文所有方向**。\n\n适用场景：\n- 判别类任务（分类、匹配、检索）\n- 特征抽取（Embedding 生成）\n- 句子理解、NLP 特征表示\n\n### 特点\n- 双向注意力（类似理解整句）\n- 不生成内容，而是**理解输入**\n- 输出通常是一个 embedding 或分类结果\n\n### 示例模型\n- **BERT / RoBERTa / ALBERT**（最典型）\n- **Sentence-BERT**（专用于向量检索）\n- **MiniLM**\n\n### 示例一句话解释\n> Encoder-only 模型擅长理解文本，但不能进行长文本生成。\n\n---\n\n## 2. Decoder-only（只用 Decoder）\n### 原理\n仅使用 **Decoder 堆叠结构**，依赖 **Masked Self-Attention（单向注意力）**。\n每个 token 只能看到 **左边的内容**，用于自回归生成。\n\n适用场景：\n- 文本生成（写作、对话、代码生成）\n- 自回归预测\n\n### 特点\n- 单向因果注意力（只看历史）\n- 自回归生成一个 token 接一个 token\n- 是 LLM（ChatGPT 类模型）使用的主流架构\n\n### 示例模型\n- **GPT 系列（GPT-2 / GPT-3 / GPT-4 / GPT-5）**\n- **LLaMA 系列**\n- **Mistral / Qwen / Doubao**\n- **OPT / BLOOM**\n\n### 示例一句话解释\n> Decoder-only 是生成型大模型的标准架构，通过因果注意力逐 token 生成输出。\n\n---\n\n## 3. 面试总结对比（表格）\n| 对比项 | Encoder-only | Decoder-only |\n|--------|--------------|---------------|\n| 注意力类型 | 双向 Self-Attention | Masked 单向 Self-Attention |\n| 主要用途 | 理解、分类、检索 | 文本生成、自回归预测 |\n| 特征 | 不能生成内容 | 擅长生成内容 |\n| 示例 | BERT | GPT |\n| 输入输出 | 输入→特征向量 | 输入→逐 token 输出 |\n\n---\n\n## 4. 面试一句话总结\n> Encoder-only（如 BERT）负责“理解”，Decoder-only（如 GPT）负责“生成”，前者是双向注意力，后者是单向因果注意力，是现代 NLP 的两大核心架构。\n"},"children":[]},{"data":{"id":"demkbpox0jk0","created":1764560027287,"text":"半监督，监督，无监督学习有什么区别，分别有什么代表的算法","expandState":"expand"},"children":[]},{"data":{"id":"demkc3eva3k0","created":1764560057154,"text":"cnn的每一层在做什么","expandState":"expand"},"children":[]},{"data":{"id":"demkc7z7aaw0","created":1764560067091,"text":"knn和kmeans聚簇有什么区别，你还用过什么别的聚簇算法","expandState":"expand"},"children":[]},{"data":{"id":"demkcqbge7k0","created":1764560107014,"text":"你怎么解决过拟合，激活函数你怎么用的","expandState":"expand"},"children":[]},{"data":{"id":"demkdomqa6g0","created":1764560181706,"text":"rnn和lstm的区别是什么，分别有什么特性","expandState":"expand"},"children":[]},{"data":{"id":"demkk0ybbsw0","created":1764560678713,"text":"pca算法有什么特性","expandState":"expand"},"children":[]},{"data":{"id":"demkkfrndb40","created":1764560710962,"text":"泊松分布和高斯分布的公式和特点","expandState":"expand"},"children":[]},{"data":{"id":"demkluyltps0","created":1764560822398,"text":"nlp是什么过程","expandState":"expand"},"children":[]},{"data":{"id":"demnem3tvqo0","created":1764568717904,"text":"描述一下kmeans的过程","expandState":"expand"},"children":[]}]},{"data":{"id":"demjus6bgpk0","created":1764558700502,"text":"手撕","expandState":"expand"},"children":[{"data":{"id":"demkf4pwuc00","created":1764560295091,"text":"二叉树反转","expandState":"expand"},"children":[]},{"data":{"id":"demkf7zszaw0","created":1764560302220,"text":"华为kmeans手撕","expandState":"expand"},"children":[]},{"data":{"id":"demkfbx3pls0","created":1764560310763,"text":"华为二叉树最大子树和","expandState":"expand"},"children":[]},{"data":{"id":"demkfjomyrc0","created":1764560327666,"text":"ip地址加.合法的话有几个ip","expandState":"expand"},"children":[]},{"data":{"id":"demkfmk6f2o0","created":1764560333926,"text":"pid和ppid kill","expandState":"expand"},"children":[]},{"data":{"id":"demkgnb92io0","created":1764560413928,"text":"循环链表，幸运儿 间隔一个淘汰","expandState":"expand"},"children":[]},{"data":{"id":"demkhxnvxiw0","created":1764560514824,"text":"找链表的交叉点  进阶考虑循环链表","expandState":"expand"},"children":[]}]},{"data":{"id":"demol2vg6ts0","created":1764572045697,"text":"HR综合"},"children":[]}]},"template":"default","theme":"fresh-blue","version":"1.4.43"}